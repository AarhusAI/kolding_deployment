vllm:
  servingEngineSpec:
    strategy:
      # We only have one GPU node, so we need to kill exiting deployment first.
      type: Recreate

    runtimeClassName: ""
    modelSpec:
      - name: "mistral"
        repository: "vllm/vllm-openai"
        tag: "v0.12.0"
        #repository: "lmcache/vllm-openai"
        #tag: "2025-05-27-v1"

        modelURL: "mistralai/Mistral-Small-3.2-24B-Instruct-2506"

        replicaCount: 1

        requestCPU: 2
        requestMemory: "56Gi"
        requestGPU: 1

        # Re-enabled: Temporary workaround for scaling up to more GPU's (currently NFS required to have RWM)
        pvcStorage: "200Gi"

        vllmConfig:
          enableChunkedPrefill: true
          enablePrefixCaching: true
          maxModelLen: 16384
          extraArgs: ["--tokenizer-mode", "mistral", "--config_format", "mistral", "--load_format", "mistral"]

        lmcacheConfig:
          enabled: false
          cpuOffloadingBufferSize: "20"

        hf_token:
          secretName: "hf-secret"
          secretKey: "TOKEN"

        nodeSelectorTerms:
          - matchExpressions:
            - key: nvidia.com/vgpu.present
              operator: "In"
              values:
                - "true"
            - key: kubernetes.io/hostname
              operator: "In"
              values:
                # Send this to the large GPU node
                - "gpu1"

      - name: "embeddings"
        repository: "vllm/vllm-openai"
        tag: "v0.12.0"

        modelURL: "intfloat/multilingual-e5-large"

        replicaCount: 1

        requestCPU: 2
        requestMemory: "16Gi"
        requestGPU: 1

        pvcStorage: "10Gi"

        vllmConfig:
          enableChunkedPrefill: false
          enablePrefixCaching: false
          maxModelLen: 512
          extraArgs: [
            # Disable cuda-graph to enable loading embedding model.
            "--compilation-config", '{"level": 0, "use_cudagraph": false}',
            "--dtype", "float16",
            "--max-num-seqs", "512",
            "--max-num-batched-tokens", "16384",
            "--gpu-memory-utilization", "0.85",
            "--disable-log-requests"
          ]

        lmcacheConfig:
          enabled: false
          cpuOffloadingBufferSize: "20"

        hf_token:
          secretName: "hf-secret"
          secretKey: "TOKEN"

        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/vgpu.present
                operator: "In"
                values:
                  - "true"
              - key: kubernetes.io/hostname
                operator: "In"
                values:
                  # Send this to the small GPU node
                  - "gpu2"

    vllmApiKey:
      secretName: "vllm-secret"
      secretKey: "KEY"

    tolerations:
    - key: "node-role.kubernetes.io/gpu"
      operator: "Exists"

  routerSpec:
    repository: "lmcache/lmstack-router"
    tag: "latest"
    imagePullPolicy: "Always"

    resources:
      requests:
        cpu: 400m
        memory: 800Mi
      limits:
        memory: 1024Mi
